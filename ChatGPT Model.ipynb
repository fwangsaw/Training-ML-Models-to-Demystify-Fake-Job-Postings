{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d2e70e-a36f-47b4-b171-6778315309b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "from apikey import client\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71105ec3-82c5-4f8e-8a7a-799d19ca4fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned dataset\n",
    "data = pd.read_csv(\"full_cleaned_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baee32d-8f3d-4a74-b2fa-794c5dbee10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Checking data length:\", len(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58e66d43-0f99-4724-824c-62e95378e47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Do sampling for cost purposes and/or test purposes for first run\n",
    "# sample_size = 1000\n",
    "# data_sample = data.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "\n",
    "descriptions = data[\"description_clean\"].tolist()\n",
    "true_labels = data[\"fraudulent\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2fa7877-fc71-4bea-bcd7-3b61ab272f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch to prevent API limit\n",
    "batch_size = 30\n",
    "max_tokens = 150\n",
    "temperature = 0\n",
    "model = \"gpt-4o\"       # change model name accordingly here, but 4o was proven the better model compared to 3.5-turbo\n",
    "delay_between_batches = 1.5\n",
    "preds = []\n",
    "failed_batches = []    # to keep track of failures\n",
    "\n",
    "# Helper function for-loop to run the API calls\n",
    "def ask_gpt_for_classification(desc_list):\n",
    "    user_content = \"\\n\\n\".join([f\"Job {i+1}:\\n{desc}\" for i, desc in enumerate(desc_list)])\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are an assistant that classifies job descriptions as either 'Fake' or 'Real'.\\n\"\n",
    "                \"Given each job description, respond with one line per job in the same order, \"\n",
    "                \"e.g. 'Job 1: Fake', 'Job 2: Real' etc.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_content\n",
    "        }\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94031f65-e78a-4b19-b8a2-1687e998a8a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(descriptions), batch_size):\n",
    "    batch = descriptions[i:i+batch_size]\n",
    "    reply = ask_gpt_for_classification(batch)\n",
    "\n",
    "    # Fail batch counter, may be able to improve (cost considerations applied)\n",
    "    if reply is None:\n",
    "        preds.extend([None] * len(batch))  # skips failed batch\n",
    "        failed_batches.append((i, batch))\n",
    "        print(f\"Batch {i//batch_size + 1} failed.\")\n",
    "        continue\n",
    "\n",
    "    lines = reply.strip().split(\"\\n\")\n",
    "    batch_preds = []\n",
    "    for line in lines:\n",
    "        if \"Fake\" in line:\n",
    "            batch_preds.append(1)\n",
    "        elif \"Real\" in line:\n",
    "            batch_preds.append(0)\n",
    "        else:\n",
    "            batch_preds.append(None)\n",
    "            \n",
    "    # pad in case of incomplete return\n",
    "    preds.extend(batch_preds)\n",
    "    print(f\"Batch {i//batch_size + 1} done ({len(preds)} total)...\")\n",
    "    time.sleep(delay_between_batches)\n",
    "\n",
    "\n",
    "print(\"Final Lengths â€” Preds:\", len(preds), \"| Labels:\", len(true_labels))\n",
    "print(f\"Failed batches: {len(failed_batches)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7353069d-9d74-4d41-bb41-793fc1a429a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch lengths: 14866 17836\n"
     ]
    }
   ],
   "source": [
    "print(\"Mismatch lengths:\", len(preds), len(true_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a42f486a-f5ed-482c-a00a-1eac16a8b6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 17836 job descriptions for GPT classification\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sampled {len(descriptions)} job descriptions for GPT classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9f95cc02-f9b0-44b3-a634-066dd46f80d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug why lengths returned mismatches original sample length\n",
    "# Theoretical answer = ChatGPT returned empty string or API failure somewhere\n",
    "# Due to cost consideration, can't rerun\n",
    "if len(batch_preds) != len(batch):\n",
    "    print(f\"Incomplete GPT return: Got {len(batch_preds)} vs Expected {len(batch)}\")\n",
    "\n",
    "# Definitely a point of improvement, potentially there are guides out there on how to methodologically run API calls better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1b0a3e21-d1ca-48f3-ae07-16890fd0b507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1}\n",
      "{0, 1}\n",
      "Total predictions: 14866 vs True Labels: 17836\n"
     ]
    }
   ],
   "source": [
    "print(set(true_labels))\n",
    "print(set(preds))\n",
    "print(f\"Total predictions: {len(preds)} vs True Labels: {len(true_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7b4eee58-3cd9-49d4-b675-b3b7b9bd372f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Classification Report:\n",
      "[[10903  3443]\n",
      " [  391   129]]\n",
      "\n",
      "Cleaned Confusion Matrix:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.76      0.85     14346\n",
      "           1       0.04      0.25      0.06       520\n",
      "\n",
      "    accuracy                           0.74     14866\n",
      "   macro avg       0.50      0.50      0.46     14866\n",
      "weighted avg       0.93      0.74      0.82     14866\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reindexing and model output results\n",
    "true_labels_partial = true_labels[:len(preds)]\n",
    "\n",
    "print(\"Cleaned Classification Report:\")\n",
    "print(confusion_matrix(true_labels_partial, preds))\n",
    "\n",
    "print(\"\\nCleaned Confusion Matrix:\")\n",
    "print(classification_report(true_labels_partial, preds))\n",
    "\n",
    "# Model shows clear imbalance between 0 (real job posts) and 1 (fake job posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "95ccaab0-9912-4659-a7ad-6fd3e34d36ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.80      0.87      6796\n",
      "           1       0.04      0.18      0.07       340\n",
      "\n",
      "    accuracy                           0.77      7136\n",
      "   macro avg       0.50      0.49      0.47      7136\n",
      "weighted avg       0.91      0.77      0.83      7136\n",
      "\n",
      "\n",
      "Cleaned Confusion Matrix:\n",
      "[[5431 1365]\n",
      " [ 279   61]]\n"
     ]
    }
   ],
   "source": [
    "# This is old chatGPT with Model 3o\n",
    "print(\"Cleaned Classification Report:\")\n",
    "print(classification_report(true_final, preds_final))\n",
    "\n",
    "print(\"\\nCleaned Confusion Matrix:\")\n",
    "print(confusion_matrix(true_final, preds_final))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
